{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "# import tim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '../spectrograms/2004'\n",
    "output_folder = '../midi-processed-values/2004'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  images, npy_arrays = zip(*batch)\n",
    "\n",
    "  # Stack images (they should all be the same size)\n",
    "  images = torch.stack(images)\n",
    "\n",
    "  # Pad NPY arrays to the same length\n",
    "  lengths = [len(npy) for npy in npy_arrays]\n",
    "  padded_npy_arrays = rnn_utils.pad_sequence(npy_arrays, batch_first=True)\n",
    "\n",
    "  return images, padded_npy_arrays, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, image_folder, npy_folder):\n",
    "        self.image_folder = image_folder\n",
    "        self.npy_folder = npy_folder\n",
    "        self.image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "        npy_path = os.path.join(self.npy_folder, image_name.replace('.png', '.midi.npy'))\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)  # Convert to CxHxW\n",
    "\n",
    "        npy_array = np.load(npy_path)\n",
    "        npy_array = torch.tensor(npy_array, dtype=torch.float32)\n",
    "\n",
    "        return image, npy_array\n",
    "\n",
    "# Example usage\n",
    "image_folder = '../spectrograms/2004'\n",
    "npy_folder = '../midi-processed-values/2004'\n",
    "dataset = SpectrogramDataset(image_folder, npy_folder)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 62 * 175, 512)  # Adjust based on input image size\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 62 * 175)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_heads, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers)\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, src, tgt, tgt_mask=None):\n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGES SHAPE:  torch.Size([64, 3, 500, 1400])\n",
      "NPY SHAPE:  torch.Size([64, 64239, 17])\n",
      "NPY AT 0 SHAPE torch.Size([64239, 17])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for images, npy_arrays, lengths in dataloader:\n",
    "        print(\"IMAGES SHAPE: \", images.shape)\n",
    "        print(\"NPY SHAPE: \", npy_arrays.shape)\n",
    "        print(\"NPY AT 0 SHAPE\", npy_arrays[0].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlos.arguedas/Desktop/NoteChaser/NoteChaser/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THESE ARE THE SEQUENCES torch.Size([64, 73394, 17])\n",
      "IMAGE TENSOR SHAPE PRE CNN torch.Size([64, 3, 500, 1400])\n",
      "IMAGE TENSOR SHAPE POST CNN torch.Size([64, 256])\n",
      "IMAGE TENSOR SHAPE POST UNSQUEEZE torch.Size([73394, 64, 256])\n",
      "SOURCE SHAPE torch.Size([73394, 64, 256])\n",
      "TARGET SHAPE torch.Size([73394, 64, 256])\n",
      "GENERATING TGT MASK...\n",
      "Done with TGT MASK.\n"
     ]
    }
   ],
   "source": [
    "# Combine CNN and Transformer\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, cnn, transformer, d_model):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.cnn = cnn\n",
    "        self.transformer = transformer\n",
    "        self.fc_tgt = nn.Linear(17, d_model)\n",
    "\n",
    "    def forward(self, image, tgt, lengths):\n",
    "        print(\"THESE ARE THE SEQUENCES\", tgt.shape)\n",
    "        print(\"IMAGE TENSOR SHAPE PRE CNN\", image.shape)\n",
    "        features = self.cnn(image)\n",
    "        print(\"IMAGE TENSOR SHAPE POST CNN\", features.shape)\n",
    "        src = features.unsqueeze(1).repeat(1, tgt.size(1), 1).permute(1, 0, 2)  # Add batch dimension for transformer\n",
    "        print(\"IMAGE TENSOR SHAPE POST UNSQUEEZE\", src.shape)\n",
    "        tgt = self.fc_tgt(tgt).permute(1, 0, 2)\n",
    "\n",
    "        print(\"SOURCE SHAPE\", src.shape)\n",
    "        print(\"TARGET SHAPE\", tgt.shape)\n",
    "\n",
    "        # Generate mask for tgt sequences\n",
    "        print(\"GENERATING TGT MASK...\")\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "        print(\"Done with TGT MASK.\")\n",
    "\n",
    "        # Pack the padded sequences\n",
    "        # packed_tgt = rnn_utils.pack_padded_sequence(tgt, lengths, batch_first=True, enforce_sorted=False)\n",
    "        # print(\"THESE ARE THE PACKED SEQUENCES\", packed_tgt.data.shape)\n",
    "\n",
    "        # print(\"RIGHT ABOUT TO ENTER THE TRANSFORMER\")\n",
    "        # print(packed_tgt.data.shape)\n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        return output\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "cnn = CNNFeatureExtractor()\n",
    "transformer = TransformerModel(input_dim=256, output_dim=17, num_heads=4, num_layers=2)\n",
    "model = CombinedModel(cnn, transformer, d_model=256)\n",
    "\n",
    "# Training loop\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    for images, npy_arrays, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, npy_arrays, lengths)\n",
    "        loss = criterion(outputs, npy_arrays)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
